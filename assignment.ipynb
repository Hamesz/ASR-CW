{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR Assignment 2020-21\n",
    "\n",
    "This notebook has been provided as a template to get you started on the assignment.  Feel free to use it for your development, or do your development directly in Python.\n",
    "\n",
    "You can find a full description of the assignment [here](http://www.inf.ed.ac.uk/teaching/courses/asr/2020-21/coursework.pdf).\n",
    "\n",
    "You are provided with two Python modules `observation_model.py` and `wer.py`.  The first was described in [Lab 3](https://github.com/Ore-an/asr_labs/blob/master/asr_lab3_4.ipynb).  The second can be used to compute the number of substitution, deletion and insertion errors between ASR output and a reference text.\n",
    "\n",
    "It can be used as follows:\n",
    "\n",
    "```python\n",
    "import wer\n",
    "\n",
    "my_refence = 'A B C'\n",
    "my_output = 'A C C D'\n",
    "\n",
    "wer.compute_alignment_errors(my_reference, my_output)\n",
    "```\n",
    "\n",
    "This produces a tuple $(s,d,i)$ giving counts of substitution,\n",
    "deletion and insertion errors respectively - in this example (1, 0, 1).  The function accepts either two strings, as in the example above, or two lists.  Matching is case sensitive.\n",
    "\n",
    "## Template code\n",
    "\n",
    "Assuming that you have already made a function to generate an WFST, `create_wfst()` and a decoder class, `MyViterbiDecoder`, you can perform recognition on all the audio files as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openfst_python as fst\n",
    "from subprocess import check_call\n",
    "from IPython.display import Image\n",
    "from helper_functions import *\n",
    "import time, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, word_table = generate_word_sequence_recognition_wfst(3, 'lexicon.txt',original=False)\n",
    "# print (f'word table :{list(word_table)}')\n",
    "# f.draw('tmp.dot', portrait=True)\n",
    "# check_call(['dot','-Tpng','-Gdpi=300','tmp.dot','-o','tmp.png'])\n",
    "# Image(filename='tmp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logger is used\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "log = logging.getLogger('root')\n",
    "FORMAT = \"%(message)s\"\n",
    "logging.basicConfig(stream=sys.stdout, format=FORMAT, datefmt=None)\n",
    "log.setLevel(logging.INFO)\n",
    "log.warning('logger is used')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import observation_model\n",
    "import math\n",
    "\n",
    "class MyViterbiDecoder:\n",
    "    \n",
    "    NLL_ZERO = 1e10  # define a constant representing -log(0).  This is really infinite, but approximate\n",
    "                     # it here with a very large number\n",
    "    \n",
    "    def __init__(self, f, audio_file_name, word_table):\n",
    "        \"\"\"Set up the decoder class with an audio file and WFST f\n",
    "        \"\"\"\n",
    "        self.om = observation_model.ObservationModel()\n",
    "        self.f = f\n",
    "        self.word_table = word_table\n",
    "        self.forward_computations = 0\n",
    "        # -- audio file\n",
    "        if audio_file_name:\n",
    "            self.om.load_audio(audio_file_name)\n",
    "        else:\n",
    "            self.om.load_dummy_audio()\n",
    "        \n",
    "        self.initialise_decoding()\n",
    "\n",
    "        \n",
    "    def initialise_decoding(self):\n",
    "        \"\"\"set up the values for V_j(0) (as negative log-likelihoods)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.V = []   # stores likelihood along best path reaching state j\n",
    "        self.B = []   # stores identity of best previous state reaching state j\n",
    "        self.W = []   # stores output labels sequence along arc reaching j - this removes need for \n",
    "                      # extra code to read the output sequence along the best path\n",
    "        \n",
    "        for t in range(self.om.observation_length()+1):\n",
    "            self.V.append([self.NLL_ZERO]*self.f.num_states())\n",
    "            self.B.append([-1]*self.f.num_states())\n",
    "            self.W.append([[] for i in range(self.f.num_states())])  #  multiplying the empty list doesn't make multiple\n",
    "        \n",
    "        # The above code means that self.V[t][j] for t = 0, ... T gives the Viterbi cost\n",
    "        # of state j, time t (in negative log-likelihood form)\n",
    "        # Initialising the costs to NLL_ZERO effectively means zero probability    \n",
    "        \n",
    "        # give the WFST start state a probability of 1.0   (NLL = 0.0)\n",
    "        self.V[0][self.f.start()] = 0.0\n",
    "        \n",
    "        # some WFSTs might have arcs with epsilon on the input (you might have already created \n",
    "        # examples of these in earlier labs) these correspond to non-emitting states, \n",
    "        # which means that we need to process them without stepping forward in time.  \n",
    "        # Don't worry too much about this!  \n",
    "        self.traverse_epsilon_arcs(0)        \n",
    "        \n",
    "    def traverse_epsilon_arcs(self, t):\n",
    "        \"\"\"Traverse arcs with <eps> on the input at time t\n",
    "        \n",
    "        These correspond to transitions that don't emit an observation\n",
    "        \n",
    "        We've implemented this function for you as it's slightly trickier than\n",
    "        the normal case.  You might like to look at it to see what's going on, but\n",
    "        don't worry if you can't fully follow it.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        states_to_traverse = list(self.f.states()) # traverse all states\n",
    "        while states_to_traverse:\n",
    "            \n",
    "            # Set i to the ID of the current state, the first \n",
    "            # item in the list (and remove it from the list)\n",
    "            i = states_to_traverse.pop(0)   \n",
    "        \n",
    "            # don't bother traversing states which have zero probability\n",
    "            if self.V[t][i] == self.NLL_ZERO:\n",
    "                    continue\n",
    "        \n",
    "            for arc in self.f.arcs(i):\n",
    "                \n",
    "                if arc.ilabel == 0:     # if <eps> transition\n",
    "                  \n",
    "                    j = arc.nextstate   # ID of next state  \n",
    "                \n",
    "                    if self.V[t][j] > self.V[t][i] + float(arc.weight):\n",
    "                        \n",
    "                        # this means we've found a lower-cost path to\n",
    "                        # state j at time t.  We might need to add it\n",
    "                        # back to the processing queue.\n",
    "                        self.V[t][j] = self.V[t][i] + float(arc.weight)\n",
    "                        \n",
    "                        # save backtrace information.  In the case of an epsilon transition, \n",
    "                        # we save the identity of the best state at t-1.  This means we may not\n",
    "                        # be able to fully recover the best path, but to do otherwise would\n",
    "                        # require a more complicated way of storing backtrace information\n",
    "                        self.B[t][j] = self.B[t][i] \n",
    "                        \n",
    "                        # and save the output labels encountered - this is a list, because\n",
    "                        # there could be multiple output labels (in the case of <eps> arcs)\n",
    "                        if arc.olabel != 0:\n",
    "                            self.W[t][j] = self.W[t][i] + [arc.olabel]\n",
    "                        else:\n",
    "                            self.W[t][j] = self.W[t][i]\n",
    "                        \n",
    "                        if j not in states_to_traverse:\n",
    "                            states_to_traverse.append(j)\n",
    "\n",
    "    \n",
    "    def forward_step_branches(self, t, branches):\n",
    "        for i in self.f.states():\n",
    "            # sort the probabilites based on lowest\n",
    "            sort_index = np.argsort(self.V[t-1])\n",
    "            #take the N (branches) best probability state\n",
    "            try:\n",
    "                best_indexs = sort_index[:branches]\n",
    "            except e:\n",
    "                best_indexs = sort_index\n",
    "                print(f\"Exception {e}\")\n",
    "            # only use best N branches\n",
    "            if (i in best_indexs):\n",
    "                for arc in self.f.arcs(i):\n",
    "                    if arc.ilabel != 0: # <eps> transitions don't emit an observation\n",
    "                        j = arc.nextstate\n",
    "                        tp = float(arc.weight)  # transition prob\n",
    "                        ep = -self.om.log_observation_probability(self.f.input_symbols().find(arc.ilabel), t)  # emission negative log prob\n",
    "                        prob = tp + ep + self.V[t-1][i] # they're logs\n",
    "                        if prob < self.V[t][j]:\n",
    "                            self.V[t][j] = prob\n",
    "                            self.B[t][j] = i\n",
    "                            # store the output labels encountered too\n",
    "                            if arc.olabel !=0:\n",
    "                                self.W[t][j] = [arc.olabel]\n",
    "                            else:\n",
    "                                self.W[t][j] = []\n",
    "\n",
    "                        # update number of forward computations\n",
    "                        self.forward_computations += 1 \n",
    "                        \n",
    "                    \n",
    "    def forward_step_moving_average(self, t, avg_moving):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            t (int): curretn timestep\n",
    "            pruning (bool):  use or not prunning \n",
    "            best_prob (int): use best previous prob or not\n",
    "            avg_moving (int): n average best probs\n",
    "            \n",
    "        \"\"\"\n",
    "#         print(f\"--- {t} ----\")\n",
    "        for i in self.f.states():\n",
    "            avg_prob_list = []\n",
    "            avg_moving = t if (avg_moving>=t) else avg_moving\n",
    "            try:\n",
    "                for t_n in range(avg_moving):\n",
    "                    avg_prob_list.append(min(self.V[t-t_n-1]))            # <!> Array out of bounds ? <!>\n",
    "                prob_considered = sum(avg_prob_list)/len(avg_prob_list)\n",
    "            except Exception as e:\n",
    "                print('t={}, t_n-1={}, t-t_n-1={}'.format(t, t_n-1, t-t_n-1))\n",
    "                raise ValueError('forward_step_moving_average: {}'.format(e))\n",
    "            success = self.V[t-1][i] <= prob_considered\n",
    "        \n",
    "            if success:\n",
    "                for arc in self.f.arcs(i):\n",
    "                    \n",
    "                    if arc.ilabel != 0: # <eps> transitions don't emit an observation\n",
    "                        j = arc.nextstate\n",
    "#                         log.info(f\"Current State: {i}, Next state: {j}\")\n",
    "                        tp = float(arc.weight)  # transition prob\n",
    "                        ep = -self.om.log_observation_probability(self.f.input_symbols().find(arc.ilabel), t)  # emission negative log prob\n",
    "                        prob = tp + ep + self.V[t-1][i] # they're logs\n",
    "                        if prob < self.V[t][j]:\n",
    "                            self.V[t][j] = prob\n",
    "                            self.B[t][j] = i\n",
    "                            \n",
    "                            # store the output labels encountered too\n",
    "                            if arc.olabel !=0:\n",
    "                                self.W[t][j] = [arc.olabel]\n",
    "                            else:\n",
    "                                self.W[t][j] = []\n",
    "                                \n",
    "                        # update number of forward computations\n",
    "                        self.forward_computations += 1\n",
    "    \n",
    "    \n",
    "    def forward_step_threshold(self, t, threshold):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            t (int): curretn timestep\n",
    "            threshold (int):  probability threshold\n",
    "        \"\"\"\n",
    "        for i in self.f.states():\n",
    "            success = self.V[t-1][i] <= threshold\n",
    "            if success:\n",
    "                for arc in self.f.arcs(i):\n",
    "                    if arc.ilabel != 0: # <eps> transitions don't emit an observation\n",
    "                        j = arc.nextstate\n",
    "                        tp = float(arc.weight)  # transition prob\n",
    "                        ep = -self.om.log_observation_probability(self.f.input_symbols().find(arc.ilabel), t)  # emission negative log prob\n",
    "                        prob = tp + ep + self.V[t-1][i] # they're logs\n",
    "                        if prob < self.V[t][j]:\n",
    "                            self.V[t][j] = prob\n",
    "                            self.B[t][j] = i\n",
    "                            # store the output labels encountered too\n",
    "                            if arc.olabel !=0:\n",
    "                                self.W[t][j] = [arc.olabel]\n",
    "                            else:\n",
    "                                self.W[t][j] = []\n",
    "                        # update number of forward computations\n",
    "                        self.forward_computations += 1\n",
    "    \n",
    "    \n",
    "    def forward_step(self, t):\n",
    "        for i in self.f.states():\n",
    "            success = self.V[t-1][i] != self.NLL_ZERO\n",
    "            if success:\n",
    "                for arc in self.f.arcs(i):\n",
    "                    if arc.ilabel != 0: # <eps> transitions don't emit an observation\n",
    "                        j = arc.nextstate\n",
    "                        tp = float(arc.weight)  # transition prob\n",
    "                        ep = -self.om.log_observation_probability(self.f.input_symbols().find(arc.ilabel), t)  # emission negative log prob\n",
    "                        prob = tp + ep + self.V[t-1][i] # they're logs\n",
    "                        if prob < self.V[t][j]:\n",
    "                            self.V[t][j] = prob\n",
    "                            self.B[t][j] = i\n",
    "                            # store the output labels encountered too\n",
    "                            if arc.olabel !=0:\n",
    "                                self.W[t][j] = [arc.olabel]\n",
    "                            else:\n",
    "                                self.W[t][j] = []\n",
    "                        # update number of forward computations\n",
    "                        self.forward_computations += 1\n",
    "    \n",
    "    \n",
    "    def finalise_decoding(self):\n",
    "        \"\"\" this incorporates the probability of terminating at each state\n",
    "        \"\"\"                \n",
    "        for state in self.f.states():\n",
    "            final_weight = float(self.f.final(state))\n",
    "            if self.V[-1][state] != self.NLL_ZERO:\n",
    "#                 print(f'state={state}; final weight = {final_weight}; prob: {self.V[-1][state]}')\n",
    "                if final_weight == math.inf:\n",
    "                    self.V[-1][state] = self.NLL_ZERO  # effectively says that we can't end in this state\n",
    "                else:\n",
    "                    self.V[-1][state] += final_weight\n",
    "        # get a list of all states where there was a path ending with non-zero probability\n",
    "        finished = [x for x in enumerate(self.V[-1]) if x[1] < self.NLL_ZERO]\n",
    "#         print(f'finished: {finished}')\n",
    "        if not finished:  # if empty\n",
    "#             print(\"No path got to the end of the observations.\")\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    def decode(self, pruning=[None,None,None]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pruning (bool):  use or not prunning \n",
    "            best_prob (int): how many to use when using best probability\n",
    "            avg_moving (int): number of states to account when using a an average\n",
    "        \"\"\"\n",
    "        self.initialise_decoding()\n",
    "        t = 1\n",
    "        while t <= self.om.observation_length():\n",
    "            \n",
    "            moving_average = pruning[0]\n",
    "            branches = pruning[1]\n",
    "            threshold = pruning[2]\n",
    "            \n",
    "            if (moving_average != None):\n",
    "                self.forward_step_moving_average(t, moving_average)\n",
    "            elif(branches != None):\n",
    "                self.forward_step_branches(t, branches)\n",
    "            elif(threshold != None):\n",
    "                self.forward_step_threshold(t, threshold)\n",
    "            else:\n",
    "#                 print(\"doing regular step forward\")\n",
    "                self.forward_step(t)\n",
    "                \n",
    "            self.traverse_epsilon_arcs(t)\n",
    "            t += 1\n",
    "        self.finalise_decoding()\n",
    "    \n",
    "    def backtrace(self):\n",
    "        \n",
    "        best_final_state = self.V[-1].index(min(self.V[-1])) # argmin\n",
    "        best_state_sequence = [best_final_state]\n",
    "        best_out_sequence = []\n",
    "        \n",
    "        t = self.om.observation_length()   # ie T\n",
    "        j = best_final_state\n",
    "        \n",
    "        while t >= 0:\n",
    "            i = self.B[t][j]\n",
    "            best_state_sequence.append(i)\n",
    "            best_out_sequence = self.W[t][j] + best_out_sequence  # computer scientists might like\n",
    "                                                                                # to make this more efficient!\n",
    "            log.debug(f\"W[t][j]: {self.W[t][j]}\")\n",
    "            # continue the backtrace at state i, time t-1\n",
    "            j = i  \n",
    "            t-=1\n",
    "            \n",
    "        best_state_sequence.reverse()\n",
    "        \n",
    "        # convert the best output sequence from FST integer labels into strings\n",
    "        best_out_sequence_str = []\n",
    "        word_strs = [x[1] for x in list(self.word_table)]\n",
    "#         log.debug(f\"word_strs: {word_strs}\")\n",
    "        for label in best_out_sequence:\n",
    "            label_str = self.f.output_symbols().find(label)\n",
    "#             log.debug(f\"label_str: {label_str}\")\n",
    "            if (label_str in word_strs):\n",
    "                best_out_sequence_str += [f'{label_str}']\n",
    "        best_out_sequence_str = ' '.join([x for x in best_out_sequence_str])\n",
    "        return (best_state_sequence, best_out_sequence_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import wer\n",
    "import observation_model\n",
    "import openfst_python as fst\n",
    "\n",
    "def read_transcription(wav_file):\n",
    "    \"\"\"\n",
    "    Get the transcription corresponding to wav_file.\n",
    "    \"\"\"\n",
    "    \n",
    "    transcription_file = os.path.splitext(wav_file)[0] + '.txt'\n",
    "    \n",
    "    with open(transcription_file, 'r') as f:\n",
    "        transcription = f.readline().strip()\n",
    "    \n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = '/group/teaching/asr/labs/individual_recordings/s1645821'\n",
    "folder = '/group/teaching/asr/labs/recordings'\n",
    "wavs_txt = [os.path.join(folder,x) for x in os.listdir(folder)]\n",
    "wavs = [wav for wav in wavs_txt if ('.wav' in wav)]\n",
    "transcripts = [wav for wav in wavs_txt if ('.txt' in wav)]\n",
    "# wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pic(wfst,name='wfst'):\n",
    "    \"\"\"\n",
    "    Generate Picture from wfst\n",
    "    \"\"\"\n",
    "    wfst.draw(name+'.dot', portrait=True)\n",
    "    check_call(['dot','-Tpng','-Gdpi=300',name+'.dot','-o',name+'.png'])\n",
    "    Image(filename=name+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 with baseline but not sure which one... think WFST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WFTS that uses 0.1 for self trans, and 0.9 for next trans, without different phones for same word\n",
    "weight_fwd, weight_self, original = 0.9, 0.1, True\n",
    "WFST_1_9_O = generate_word_sequence_recognition_wfst(3, 'lexicon.txt',original, weight_fwd, weight_self)\n",
    "# to_pic(WFST_1_9_O[0],name='WFST_pics/WFST_1_9_original')\n",
    "\n",
    "# WFTS that uses 0.1 for self trans, and 0.9 for next trans, with different phones for same word\n",
    "weight_fwd, weight_self, original = 0.9, 0.1, False\n",
    "WFST_1_9 = generate_word_sequence_recognition_wfst(3, 'lexicon.txt',original, weight_fwd, weight_self)\n",
    "# to_pic(WFST_1_9[0],name='WFST_pics/WFST_1_9_multi_word')\n",
    "\n",
    "# WFST that doesnt use probs all trans are None but is original\n",
    "weight_fwd, weight_self, original = None, None, True\n",
    "WFST_BASELINE = generate_word_sequence_recognition_wfst(3, 'lexicon.txt',original, weight_fwd, weight_self)\n",
    "# to_pic(WFST_O[0],name='WFST_pics/WFST_BASELINE')\n",
    "# WFTS that uses 0.5 for self trans, and 0.5 for next trans, without different phones for same word\n",
    "# weight_fwd, weight_self, original = 0.5, 0.5, True\n",
    "# WFST_5_5_O = generate_word_sequence_recognition_wfst(3, 'lexicon.txt',original, weight_fwd, weight_self)\n",
    "\n",
    "# WFST that doesnt use probs all trans are None but is not original\n",
    "weight_fwd, weight_self, original = None, None, False\n",
    "WFST_multi = generate_word_sequence_recognition_wfst(3, 'lexicon.txt',original, weight_fwd, weight_self)\n",
    "# to_pic(WFST_multi[0],name='WFST_pics/WFST_0_5_multi_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, _ = WFST_1_9_O\n",
    "# for s in f.states():\n",
    "#     if (float(f.final(s)) != math.inf):\n",
    "#         print(f\"State {s} is a final State!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2028e3a9076e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mdecoder_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpruning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpruning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mdecoder_end_time\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mstate_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbacktrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# you'll need to modify the backtrace() from Lab 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c4c96db95e91>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, pruning)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_step_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraverse_epsilon_arcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c4c96db95e91>\u001b[0m in \u001b[0;36mforward_step\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLL_ZERO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0marc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0marc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0milabel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# <eps> transitions don't emit an observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "total_error_counts = []\n",
    "total_word_counts = []\n",
    "\n",
    "total_average_decoder_times = []\n",
    "total_average_backtrace_times = []\n",
    "\n",
    "total_forward_computations = []\n",
    "\n",
    "different_fs_word_table = [WFST_1_9_O, WFST_1_9, WFST_BASELINE, WFST_multi]\n",
    "\n",
    "for idx, (f, word_table) in enumerate(different_fs_word_table[:]):\n",
    "    f_error_counts = [0,0,0]\n",
    "    f_word_counts = 0\n",
    "    decoder_times = []\n",
    "    backtrace_times = []\n",
    "    forward_computations = []\n",
    "    # -- Pruning -- Default False, False, None \n",
    "    pruning = [None,None,None]\n",
    "    \n",
    "    for wav_file in glob.glob('/group/teaching/asr/labs/recordings/*.wav')[:]:    # replace path if using your own\n",
    "        \n",
    "        decoder = MyViterbiDecoder(f, wav_file, word_table)\n",
    "        \n",
    "        decoder_start_time = time.time()\n",
    "        decoder.decode(pruning=pruning)\n",
    "        decoder_end_time  = time.time()\n",
    "        (state_path, words) = decoder.backtrace()  # you'll need to modify the backtrace() from Lab 4\n",
    "                                           # to return the words along the best path\n",
    "        backtrace_end_time = time.time()\n",
    "        \n",
    "        transcription = read_transcription(wav_file)\n",
    "        error_counts = wer.compute_alignment_errors(transcription, words) #num_subs, num_del, num_ins\n",
    "        word_count = len(transcription.split())\n",
    "        \n",
    "        # add up output\n",
    "        f_word_counts += word_count\n",
    "        f_error_counts[0] += error_counts[0]\n",
    "        f_error_counts[1] += error_counts[1]\n",
    "        f_error_counts[2] += error_counts[2]\n",
    "#         print(error_counts, word_count)     # you'll need to accumulate these to produce an overall Word Error Rate\n",
    "        \n",
    "        # add up times\n",
    "        decoder_time = decoder_end_time - decoder_start_time\n",
    "        backtrace_time = backtrace_end_time - decoder_end_time\n",
    "        \n",
    "        decoder_times.append(decoder_time)\n",
    "        backtrace_times.append(backtrace_time)\n",
    "        \n",
    "        # add up conputations\n",
    "        forward_computations.append(decoder.forward_computations)\n",
    "        \n",
    "        # -- add to DataFrame\n",
    "#         print(f\"WFTS {idx}: Transcription: {transcription}\\nWords: {words}\")\n",
    "        \n",
    "    total_error_counts.append(f_error_counts)\n",
    "    total_word_counts.append(f_word_counts)\n",
    "    \n",
    "    total_average_decoder_times.append(sum(decoder_times)/len(decoder_times))\n",
    "    total_average_backtrace_times.append(sum(backtrace_times)/len(backtrace_times))\n",
    "#     print(f\"Forward computations for fst {idx}: {forward_computations}\")\n",
    "    total_forward_computations.append(sum(forward_computations)/len(forward_computations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns=['WFST', 'S', 'D', 'I', 'Accuracy','Word Counts', 'Average Decoder Times', 'Average Backtrace Times', 'Average Forward Computations']\n",
    "WFSTs = ['baseline log', 'multi word log', 'None weight original', 'None weight multi word']\n",
    "total_error_counts = np.array(total_error_counts)\n",
    "subs = total_error_counts[:,0]\n",
    "deletions = total_error_counts[:,1]\n",
    "insertions = total_error_counts[:,2]\n",
    "accuracies = ((subs + deletions + insertions)/total_word_counts)*100\n",
    "df = pd.DataFrame((WFSTs, subs, deletions, insertions, accuracies, total_word_counts, total_average_decoder_times, total_average_backtrace_times, total_forward_computations),index=columns).T\n",
    "# df.to_excel('task1.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dict_first, dict_last, dict_all) = get_word_occurences(transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WFST with <b>final probabilities</b> determined by how often that word is the last word in the list of transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WFST with final probabilities based of occurance of word being last in transcript\n",
    "weight_fwd, weight_self, original = None, None, True\n",
    "final_prob_dict = calculate_probabilities(dict_last)\n",
    "WFST_final_prob = generate_WFST_final_probability(3, 'lexicon.txt', 0.5, 0.5, final_prob_dict, original=original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pic(WFST_final_prob[0],name='WFST_final_probability_0.5_0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WFST with the <b>self loop and transition probabilities</b> changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WFTS that uses 0.5 for self trans, and 0.5 for next trans, without different phones for same word\n",
    "weight_fwd, weight_self, original = 0.3, 0.7, True\n",
    "WFST_3_7 = generate_word_sequence_recognition_wfst(3, 'lexicon.txt',original, weight_fwd, weight_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pic(WFST_3_7[0],name='WFST_3_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WFST with <b>unigram probabilites</b> which states how likely you are to enter the the states of the word based on its occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WFST with word probabilities (unigram)\n",
    "weight_fwd, weight_self, original = 0.5,0.5, True\n",
    "prob_dict = calculate_probabilities(dict_all)\n",
    "WFST_unigram = generate_WFST_unigram(3, 'lexicon.txt', weight_fwd, weight_self, prob_dict, original=original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pic(WFST_unigram[0],name='WFST_unigram_0.5_0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WFST that has <b>silent states</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WFST with word probabilities (unigram)\n",
    "weight_fwd, weight_self, original = 0.5,0.5, True\n",
    "WFST_silent= generate_WFST_silent(3, 'lexicon.txt', weight_fwd, weight_self, original=original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pic(WFST_silent[0],name='WFST_silent_0.5_0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_error_counts = []\n",
    "total_word_counts = []\n",
    "\n",
    "total_average_decoder_times = []\n",
    "total_average_backtrace_times = []\n",
    "\n",
    "total_forward_computations = []\n",
    "\n",
    "different_fs_word_table = [WFST_final_prob, WFST_unigram, WFST_silent]\n",
    "\n",
    "for f, word_table in different_fs_word_table[:]:\n",
    "    f_error_counts = [0,0,0]\n",
    "    f_word_counts = 0\n",
    "    decoder_times = []\n",
    "    backtrace_times = []\n",
    "    forward_computations = []    \n",
    "    \n",
    "    for wav_file in glob.glob('/group/teaching/asr/labs/recordings/*.wav')[:]:    # replace path if using your own\n",
    "        \n",
    "        decoder = MyViterbiDecoder(f, wav_file, word_table)\n",
    "        \n",
    "        decoder_start_time = time.time()\n",
    "        decoder.decode()\n",
    "        decoder_end_time  = time.time()\n",
    "        (state_path, words) = decoder.backtrace()  # you'll need to modify the backtrace() from Lab 4\n",
    "                                           # to return the words along the best path\n",
    "        backtrace_end_time = time.time()\n",
    "        \n",
    "        transcription = read_transcription(wav_file)\n",
    "        \n",
    "        # remove silence from words as this doesnt count in transcription\n",
    "        if ('<silence>' in words):\n",
    "            words = words.replace('<silence>', '')\n",
    "            words = words[1:-1]\n",
    "        \n",
    "        error_counts = wer.compute_alignment_errors(transcription, words) #num_subs, num_del, num_ins\n",
    "        word_count = len(transcription.split())\n",
    "        \n",
    "#         print(f\"Transcription: {transcription}\\nWords: {words}\")\n",
    "        \n",
    "        # add up output\n",
    "        f_word_counts += word_count\n",
    "        f_error_counts[0] += error_counts[0]\n",
    "        f_error_counts[1] += error_counts[1]\n",
    "        f_error_counts[2] += error_counts[2]\n",
    "#         print(error_counts, word_count)     # you'll need to accumulate these to produce an overall Word Error Rate\n",
    "        \n",
    "        # add up times\n",
    "        decoder_time = decoder_end_time - decoder_start_time\n",
    "        backtrace_time = backtrace_end_time - decoder_end_time\n",
    "        \n",
    "        decoder_times.append(decoder_time)\n",
    "        backtrace_times.append(backtrace_time)\n",
    "        \n",
    "        # add up conputations\n",
    "        forward_computations.append(decoder.forward_computations)\n",
    "        \n",
    "        # -- add to DataFrame\n",
    "        wav_name = wav_file.split('\\\\')[-1]\n",
    "        pd_row = []\n",
    "        \n",
    "    total_error_counts.append(f_error_counts)\n",
    "    total_word_counts.append(f_word_counts)\n",
    "    \n",
    "    total_average_decoder_times.append(sum(decoder_times)/len(decoder_times))\n",
    "    total_average_backtrace_times.append(sum(backtrace_times)/len(backtrace_times))\n",
    "    \n",
    "    total_forward_computations.append(sum(forward_computations)/len(forward_computations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['WFST', 'S', 'D', 'I', 'Accuracy','Word Counts', 'Decoder Times', 'Backtrace Times', 'Forward Computations']\n",
    "WFSTs = ['final probabilities changed', 'Unigram','Silent']\n",
    "total_error_counts = np.array(total_error_counts)\n",
    "subs = total_error_counts[:,0]\n",
    "deletions = total_error_counts[:,1]\n",
    "insertions = total_error_counts[:,2]\n",
    "accuracies = ((subs + deletions + insertions)/total_word_counts)*100\n",
    "df = pd.DataFrame((WFSTs, subs, deletions, insertions, accuracies, total_word_counts, total_average_decoder_times, total_average_backtrace_times, total_forward_computations),index=columns).T\n",
    "df.to_excel('task2_wfts_with_0_5.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WFST</th>\n",
       "      <th>S</th>\n",
       "      <th>D</th>\n",
       "      <th>I</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Word Counts</th>\n",
       "      <th>Decoder Times</th>\n",
       "      <th>Backtrace Times</th>\n",
       "      <th>Forward Computations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>final probabilities changed</td>\n",
       "      <td>439</td>\n",
       "      <td>60</td>\n",
       "      <td>806</td>\n",
       "      <td>61.009818</td>\n",
       "      <td>2139</td>\n",
       "      <td>2.416013</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>76126.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unigram</td>\n",
       "      <td>423</td>\n",
       "      <td>76</td>\n",
       "      <td>746</td>\n",
       "      <td>58.204769</td>\n",
       "      <td>2139</td>\n",
       "      <td>2.371623</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>76126.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Silent</td>\n",
       "      <td>491</td>\n",
       "      <td>100</td>\n",
       "      <td>278</td>\n",
       "      <td>40.626461</td>\n",
       "      <td>2139</td>\n",
       "      <td>2.487561</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>81257.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          WFST    S    D    I   Accuracy Word Counts  \\\n",
       "0  final probabilities changed  439   60  806  61.009818        2139   \n",
       "1                      Unigram  423   76  746  58.204769        2139   \n",
       "2                       Silent  491  100  278  40.626461        2139   \n",
       "\n",
       "  Decoder Times Backtrace Times Forward Computations  \n",
       "0      2.416013        0.000838              76126.5  \n",
       "1      2.371623        0.000755              76126.5  \n",
       "2      2.487561        0.000752              81257.4  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Try different self loop weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]\n"
     ]
    }
   ],
   "source": [
    "self_loops = np.arange(0.1,1,0.1)\n",
    "print(self_loops)\n",
    "varying_loop_probs_WFSTs_Word_table = []\n",
    "for self_loop_prob in self_loops:\n",
    "    # create WFST\n",
    "    weight_fwd, weight_self, original = 1 - self_loop_prob, self_loop_prob, True\n",
    "    wfst_table = generate_word_sequence_recognition_wfst(3, 'lexicon.txt',original, weight_fwd, weight_self)\n",
    "    # add it to loop\n",
    "    varying_loop_probs_WFSTs_Word_table.append(wfst_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_error_counts = []\n",
    "total_word_counts = []\n",
    "\n",
    "total_average_decoder_times = []\n",
    "total_average_backtrace_times = []\n",
    "\n",
    "total_forward_computations = []\n",
    "\n",
    "for f, word_table in varying_loop_probs_WFSTs_Word_table[:]:\n",
    "    f_error_counts = [0,0,0]\n",
    "    f_word_counts = 0\n",
    "    decoder_times = []\n",
    "    backtrace_times = []\n",
    "    forward_computations = []    \n",
    "    \n",
    "    for wav_file in glob.glob('/group/teaching/asr/labs/recordings/*.wav')[:]:    # replace path if using your own\n",
    "        \n",
    "        decoder = MyViterbiDecoder(f, wav_file, word_table)\n",
    "        \n",
    "        decoder_start_time = time.time()\n",
    "        decoder.decode()\n",
    "        decoder_end_time  = time.time()\n",
    "        (state_path, words) = decoder.backtrace()  # you'll need to modify the backtrace() from Lab 4\n",
    "                                           # to return the words along the best path\n",
    "        backtrace_end_time = time.time()\n",
    "        \n",
    "        transcription = read_transcription(wav_file)\n",
    "        \n",
    "        # remove silence from words as this doesnt count in transcription\n",
    "#         if ('<silence>' in words):\n",
    "#             words = words.replace('<silence>', '')\n",
    "#             words = words[1:-1]\n",
    "        \n",
    "        error_counts = wer.compute_alignment_errors(transcription, words) #num_subs, num_del, num_ins\n",
    "        word_count = len(transcription.split())\n",
    "        \n",
    "#         print(f\"Transcription: {transcription}\\nWords: {words}\")\n",
    "        \n",
    "        # add up output\n",
    "        f_word_counts += word_count\n",
    "        f_error_counts[0] += error_counts[0]\n",
    "        f_error_counts[1] += error_counts[1]\n",
    "        f_error_counts[2] += error_counts[2]\n",
    "#         print(error_counts, word_count)     # you'll need to accumulate these to produce an overall Word Error Rate\n",
    "        \n",
    "        # add up times\n",
    "        decoder_time = decoder_end_time - decoder_start_time\n",
    "        backtrace_time = backtrace_end_time - decoder_end_time\n",
    "        \n",
    "        decoder_times.append(decoder_time)\n",
    "        backtrace_times.append(backtrace_time)\n",
    "        \n",
    "        # add up conputations\n",
    "        forward_computations.append(decoder.forward_computations)\n",
    "        \n",
    "        # -- add to DataFrame\n",
    "        wav_name = wav_file.split('\\\\')[-1]\n",
    "        pd_row = []\n",
    "        \n",
    "    total_error_counts.append(f_error_counts)\n",
    "    total_word_counts.append(f_word_counts)\n",
    "    \n",
    "    total_average_decoder_times.append(sum(decoder_times)/len(decoder_times))\n",
    "    total_average_backtrace_times.append(sum(backtrace_times)/len(backtrace_times))\n",
    "    \n",
    "    total_forward_computations.append(sum(forward_computations)/len(forward_computations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WFST</th>\n",
       "      <th>S</th>\n",
       "      <th>D</th>\n",
       "      <th>I</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Word Counts</th>\n",
       "      <th>Decoder Times</th>\n",
       "      <th>Backtrace Times</th>\n",
       "      <th>Forward Computations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>463.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>112.669472</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.300147</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>76126.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>447.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>82.561945</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.310659</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>76126.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>439.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>70.687237</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.316210</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>76126.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>435.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>901.0</td>\n",
       "      <td>65.170640</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.313778</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>76126.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>441.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>61.430575</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.259490</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>76126.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>436.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>58.672277</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.244737</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>76126.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>425.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>55.352969</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.323764</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>76126.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>422.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>53.436185</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.298683</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>76126.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>426.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>547.0</td>\n",
       "      <td>50.724638</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.282567</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>76126.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   WFST      S      D       I    Accuracy  Word Counts  Decoder Times  \\\n",
       "0   0.1  463.0   15.0  1932.0  112.669472       2139.0       2.300147   \n",
       "1   0.2  447.0   39.0  1280.0   82.561945       2139.0       2.310659   \n",
       "2   0.3  439.0   51.0  1022.0   70.687237       2139.0       2.316210   \n",
       "3   0.4  435.0   58.0   901.0   65.170640       2139.0       2.313778   \n",
       "4   0.5  441.0   60.0   813.0   61.430575       2139.0       2.259490   \n",
       "5   0.6  436.0   72.0   747.0   58.672277       2139.0       2.244737   \n",
       "6   0.7  425.0   83.0   676.0   55.352969       2139.0       2.323764   \n",
       "7   0.8  422.0   94.0   627.0   53.436185       2139.0       2.298683   \n",
       "8   0.9  426.0  112.0   547.0   50.724638       2139.0       2.282567   \n",
       "\n",
       "   Backtrace Times  Forward Computations  \n",
       "0         0.000828               76126.5  \n",
       "1         0.000805               76126.5  \n",
       "2         0.000890               76126.5  \n",
       "3         0.000759               76126.5  \n",
       "4         0.000758               76126.5  \n",
       "5         0.000750               76126.5  \n",
       "6         0.000736               76126.5  \n",
       "7         0.000762               76126.5  \n",
       "8         0.000699               76126.5  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=['WFST', 'S', 'D', 'I', 'Accuracy','Word Counts', 'Decoder Times', 'Backtrace Times', 'Forward Computations']\n",
    "WFSTs = self_loops\n",
    "total_error_counts = np.array(total_error_counts)\n",
    "subs = total_error_counts[:,0]\n",
    "deletions = total_error_counts[:,1]\n",
    "insertions = total_error_counts[:,2]\n",
    "accuracies = ((subs + deletions + insertions)/total_word_counts)*100\n",
    "df = pd.DataFrame((WFSTs, subs, deletions, insertions, accuracies, total_word_counts, total_average_decoder_times, total_average_backtrace_times, total_forward_computations),index=columns).T\n",
    "df.to_excel('task2_vary_self_loop.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taks 3- Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Branchse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets the test the branches, going from 1 -> f.num_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches = WFST_O[0].num_states()\n",
    "\n",
    "total_error_counts = []\n",
    "total_word_counts = []\n",
    "\n",
    "total_average_decoder_times = []\n",
    "total_average_backtrace_times = []\n",
    "\n",
    "total_forward_computations = []\n",
    "\n",
    "different_fs_word_table = [WFST_BASELINE]\n",
    "\n",
    "for n in range(1, branches + 1)[::5]:\n",
    "    for idx, (f, word_table) in enumerate(different_fs_word_table[:1]):\n",
    "        f_error_counts = [0,0,0]\n",
    "        f_word_counts = 0\n",
    "        decoder_times = []\n",
    "        backtrace_times = []\n",
    "        forward_computations = []\n",
    "        # -- Pruning -- Default False, False, None \n",
    "        pruning = [None,n,None]\n",
    "\n",
    "        for wav_file in glob.glob('/group/teaching/asr/labs/recordings/*.wav')[:]:    # replace path if using your own\n",
    "\n",
    "            decoder = MyViterbiDecoder(f, wav_file, word_table)\n",
    "\n",
    "            decoder_start_time = time.time()\n",
    "            decoder.decode(pruning=pruning)\n",
    "            decoder_end_time  = time.time()\n",
    "            (state_path, words) = decoder.backtrace()  # you'll need to modify the backtrace() from Lab 4\n",
    "                                               # to return the words along the best path\n",
    "            backtrace_end_time = time.time()\n",
    "\n",
    "            transcription = read_transcription(wav_file)\n",
    "            error_counts = wer.compute_alignment_errors(transcription, words) #num_subs, num_del, num_ins\n",
    "            word_count = len(transcription.split())\n",
    "\n",
    "            # add up output\n",
    "            f_word_counts += word_count\n",
    "            f_error_counts[0] += error_counts[0]\n",
    "            f_error_counts[1] += error_counts[1]\n",
    "            f_error_counts[2] += error_counts[2]\n",
    "    #         print(error_counts, word_count)     # you'll need to accumulate these to produce an overall Word Error Rate\n",
    "\n",
    "            # add up times\n",
    "            decoder_time = decoder_end_time - decoder_start_time\n",
    "            backtrace_time = backtrace_end_time - decoder_end_time\n",
    "\n",
    "            decoder_times.append(decoder_time)\n",
    "            backtrace_times.append(backtrace_time)\n",
    "\n",
    "            # add up conputations\n",
    "            forward_computations.append(decoder.forward_computations)\n",
    "\n",
    "            # -- add to DataFrame\n",
    "#             print(f\"WFTS {idx}: Transcription: {transcription}\\nWords: {words}\")\n",
    "        \n",
    "    total_error_counts.append(f_error_counts)\n",
    "    total_word_counts.append(f_word_counts)\n",
    "    \n",
    "    total_average_decoder_times.append(sum(decoder_times)/len(decoder_times))\n",
    "    total_average_backtrace_times.append(sum(backtrace_times)/len(backtrace_times))\n",
    "#     print(f\"Forward computations for fst {idx}: {forward_computations}\")\n",
    "    total_forward_computations.append(sum(forward_computations)/len(forward_computations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['N Branches', 'S', 'D', 'I', 'Accuracy','Word Counts', 'Decoder Times', 'Backtrace Times', 'Forward Computations']\n",
    "WFSTs = range(1,branches + 1)[::5]\n",
    "total_error_counts = np.array(total_error_counts)\n",
    "subs = total_error_counts[:,0]\n",
    "deletions = total_error_counts[:,1]\n",
    "insertions = total_error_counts[:,2]\n",
    "accuracies = ((subs + deletions + insertions)/total_word_counts)*100\n",
    "df = pd.DataFrame((WFSTs, subs, deletions, insertions, accuracies, total_word_counts, total_average_decoder_times, total_average_backtrace_times, total_forward_computations),index=columns).T\n",
    "# df.to_excel('Pruning_branches.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N Branches</th>\n",
       "      <th>S</th>\n",
       "      <th>D</th>\n",
       "      <th>I</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Word Counts</th>\n",
       "      <th>Decoder Times</th>\n",
       "      <th>Backtrace Times</th>\n",
       "      <th>Forward Computations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.764840</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>93.174381</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.978422</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>4309.671429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.0</td>\n",
       "      <td>733.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>85.647499</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>1.189320</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>7705.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>573.0</td>\n",
       "      <td>78.962132</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>1.300108</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>11133.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>782.0</td>\n",
       "      <td>76.157083</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>1.451541</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>14640.885714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26.0</td>\n",
       "      <td>578.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>833.0</td>\n",
       "      <td>74.614306</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>1.574546</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>18174.221429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>72.276765</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>1.702304</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>21725.364286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>36.0</td>\n",
       "      <td>537.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>828.0</td>\n",
       "      <td>69.191211</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>1.795292</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>25282.557143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>41.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>856.0</td>\n",
       "      <td>68.536699</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>1.918278</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>28813.442857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>46.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>853.0</td>\n",
       "      <td>66.386162</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.023924</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>32304.914286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>51.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>65.077139</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.165802</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>35790.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>56.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>841.0</td>\n",
       "      <td>64.188873</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.311520</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>39267.471429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>61.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>846.0</td>\n",
       "      <td>64.282375</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.357211</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>42747.135714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>66.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>844.0</td>\n",
       "      <td>63.674614</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.433977</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>46180.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>71.0</td>\n",
       "      <td>459.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>63.066854</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.557768</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>49596.314286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>76.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>835.0</td>\n",
       "      <td>62.926601</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.658164</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>53019.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>81.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>831.0</td>\n",
       "      <td>62.412342</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.752535</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>56421.457143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>86.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>827.0</td>\n",
       "      <td>62.272090</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.847267</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>59753.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>91.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>818.0</td>\n",
       "      <td>61.617578</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.949244</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>62959.278571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>96.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>61.617578</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>3.036669</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>66154.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>101.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>815.0</td>\n",
       "      <td>61.524077</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>3.137181</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>69331.957143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>106.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>815.0</td>\n",
       "      <td>61.524077</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>3.357000</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>72328.114286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>111.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>61.430575</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>3.405639</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>75068.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>116.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>61.430575</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>3.489814</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>77308.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    N Branches      S       D      I    Accuracy  Word Counts  Decoder Times  \\\n",
       "0          1.0    0.0  2139.0    0.0  100.000000       2139.0       0.764840   \n",
       "1          6.0  556.0  1240.0  197.0   93.174381       2139.0       0.978422   \n",
       "2         11.0  733.0   730.0  369.0   85.647499       2139.0       1.189320   \n",
       "3         16.0  624.0   492.0  573.0   78.962132       2139.0       1.300108   \n",
       "4         21.0  606.0   241.0  782.0   76.157083       2139.0       1.451541   \n",
       "5         26.0  578.0   185.0  833.0   74.614306       2139.0       1.574546   \n",
       "6         31.0  586.0   124.0  836.0   72.276765       2139.0       1.702304   \n",
       "7         36.0  537.0   115.0  828.0   69.191211       2139.0       1.795292   \n",
       "8         41.0  515.0    95.0  856.0   68.536699       2139.0       1.918278   \n",
       "9         46.0  492.0    75.0  853.0   66.386162       2139.0       2.023924   \n",
       "10        51.0  486.0    61.0  845.0   65.077139       2139.0       2.165802   \n",
       "11        56.0  472.0    60.0  841.0   64.188873       2139.0       2.311520   \n",
       "12        61.0  473.0    56.0  846.0   64.282375       2139.0       2.357211   \n",
       "13        66.0  462.0    56.0  844.0   63.674614       2139.0       2.433977   \n",
       "14        71.0  459.0    54.0  836.0   63.066854       2139.0       2.557768   \n",
       "15        76.0  455.0    56.0  835.0   62.926601       2139.0       2.658164   \n",
       "16        81.0  446.0    58.0  831.0   62.412342       2139.0       2.752535   \n",
       "17        86.0  446.0    59.0  827.0   62.272090       2139.0       2.847267   \n",
       "18        91.0  440.0    60.0  818.0   61.617578       2139.0       2.949244   \n",
       "19        96.0  441.0    60.0  817.0   61.617578       2139.0       3.036669   \n",
       "20       101.0  441.0    60.0  815.0   61.524077       2139.0       3.137181   \n",
       "21       106.0  441.0    60.0  815.0   61.524077       2139.0       3.357000   \n",
       "22       111.0  441.0    60.0  813.0   61.430575       2139.0       3.405639   \n",
       "23       116.0  441.0    60.0  813.0   61.430575       2139.0       3.489814   \n",
       "\n",
       "    Backtrace Times  Forward Computations  \n",
       "0          0.000537              0.000000  \n",
       "1          0.000571           4309.671429  \n",
       "2          0.000605           7705.600000  \n",
       "3          0.000654          11133.678571  \n",
       "4          0.000654          14640.885714  \n",
       "5          0.000708          18174.221429  \n",
       "6          0.000677          21725.364286  \n",
       "7          0.000700          25282.557143  \n",
       "8          0.000645          28813.442857  \n",
       "9          0.000701          32304.914286  \n",
       "10         0.000739          35790.150000  \n",
       "11         0.000794          39267.471429  \n",
       "12         0.000713          42747.135714  \n",
       "13         0.000667          46180.357143  \n",
       "14         0.000680          49596.314286  \n",
       "15         0.000679          53019.685714  \n",
       "16         0.000664          56421.457143  \n",
       "17         0.000652          59753.242857  \n",
       "18         0.000650          62959.278571  \n",
       "19         0.000636          66154.800000  \n",
       "20         0.000660          69331.957143  \n",
       "21         0.000756          72328.114286  \n",
       "22         0.000743          75068.650000  \n",
       "23         0.000772          77308.500000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2aabbd78f350>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(1,df.shape[0]+1))\n",
    "y = df['Accuracy'].values\n",
    "\n",
    "plt.xlabel(\"Branches\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = 10\n",
    "\n",
    "total_error_counts = []\n",
    "total_word_counts = []\n",
    "\n",
    "total_average_decoder_times = []\n",
    "total_average_backtrace_times = []\n",
    "\n",
    "total_forward_computations = []\n",
    "\n",
    "different_fs_word_table = [WFST_O]\n",
    "\n",
    "for n in range(1, averages + 1):\n",
    "    for idx, (f, word_table) in enumerate(different_fs_word_table[:]):\n",
    "        f_error_counts = [0,0,0]\n",
    "        f_word_counts = 0\n",
    "        decoder_times = []\n",
    "        backtrace_times = []\n",
    "        forward_computations = []\n",
    "        # -- Pruning -- Default False, False, None \n",
    "        pruning = [n,None,None]\n",
    "\n",
    "        for wav_file in glob.glob('/group/teaching/asr/labs/recordings/*.wav')[:1]:    # replace path if using your own\n",
    "\n",
    "            decoder = MyViterbiDecoder(f, wav_file, word_table)\n",
    "\n",
    "            decoder_start_time = time.time()\n",
    "            decoder.decode(pruning=pruning)\n",
    "            decoder_end_time  = time.time()\n",
    "            (state_path, words) = decoder.backtrace()  # you'll need to modify the backtrace() from Lab 4\n",
    "                                               # to return the words along the best path\n",
    "            backtrace_end_time = time.time()\n",
    "\n",
    "            transcription = read_transcription(wav_file)\n",
    "            error_counts = wer.compute_alignment_errors(transcription, words) #num_subs, num_del, num_ins\n",
    "            word_count = len(transcription.split())\n",
    "\n",
    "            # add up output\n",
    "            f_word_counts += word_count\n",
    "            f_error_counts[0] += error_counts[0]\n",
    "            f_error_counts[1] += error_counts[1]\n",
    "            f_error_counts[2] += error_counts[2]\n",
    "    #         print(error_counts, word_count)     # you'll need to accumulate these to produce an overall Word Error Rate\n",
    "\n",
    "            # add up times\n",
    "            decoder_time = decoder_end_time - decoder_start_time\n",
    "            backtrace_time = backtrace_end_time - decoder_end_time\n",
    "\n",
    "            decoder_times.append(decoder_time)\n",
    "            backtrace_times.append(backtrace_time)\n",
    "\n",
    "            # add up conputations\n",
    "            forward_computations.append(decoder.forward_computations)\n",
    "\n",
    "            # -- add to DataFrame\n",
    "#             print(f\"WFTS {idx}: Transcription: {transcription}\\nWords: {words}\")\n",
    "        \n",
    "    total_error_counts.append(f_error_counts)\n",
    "    total_word_counts.append(f_word_counts)\n",
    "    \n",
    "    total_average_decoder_times.append(sum(decoder_times)/len(decoder_times))\n",
    "    total_average_backtrace_times.append(sum(backtrace_times)/len(backtrace_times))\n",
    "#     print(f\"Forward computations for fst {idx}: {forward_computations}\")\n",
    "    total_forward_computations.append(sum(forward_computations)/len(forward_computations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['N Averages', 'S', 'D', 'I', 'Accuracy','Word Counts', 'Decoder Times', 'Backtrace Times', 'Forward Computations']\n",
    "WFSTs = range(1,averages + 1)\n",
    "total_error_counts = np.array(total_error_counts)\n",
    "subs = total_error_counts[:,0]\n",
    "deletions = total_error_counts[:,1]\n",
    "insertions = total_error_counts[:,2]\n",
    "accuracies = ((subs + deletions + insertions)/total_word_counts)*100\n",
    "df = pd.DataFrame((WFSTs, subs, deletions, insertions, accuracies, total_word_counts, total_average_decoder_times, total_average_backtrace_times, total_forward_computations),index=columns).T\n",
    "# df.to_excel('Pruning_branches.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N Averages</th>\n",
       "      <th>S</th>\n",
       "      <th>D</th>\n",
       "      <th>I</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Word Counts</th>\n",
       "      <th>Decoder Times</th>\n",
       "      <th>Backtrace Times</th>\n",
       "      <th>Forward Computations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.369920</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>3376.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.749055</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>68900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.836667</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>68690.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.954034</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>68480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.048860</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>68270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.158566</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>68060.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.174804</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>67850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.268159</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>67640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.346009</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>67430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.441619</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>67220.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N Averages    S    D    I    Accuracy  Word Counts  Decoder Times  \\\n",
       "0         1.0  4.0  2.0  0.0   85.714286          7.0       0.369920   \n",
       "1         2.0  0.0  7.0  0.0  100.000000          7.0       1.749055   \n",
       "2         3.0  0.0  7.0  0.0  100.000000          7.0       1.836667   \n",
       "3         4.0  0.0  7.0  0.0  100.000000          7.0       1.954034   \n",
       "4         5.0  0.0  7.0  0.0  100.000000          7.0       2.048860   \n",
       "5         6.0  0.0  7.0  0.0  100.000000          7.0       2.158566   \n",
       "6         7.0  0.0  7.0  0.0  100.000000          7.0       2.174804   \n",
       "7         8.0  0.0  7.0  0.0  100.000000          7.0       2.268159   \n",
       "8         9.0  0.0  7.0  0.0  100.000000          7.0       2.346009   \n",
       "9        10.0  0.0  7.0  0.0  100.000000          7.0       2.441619   \n",
       "\n",
       "   Backtrace Times  Forward Computations  \n",
       "0         0.000438                3376.0  \n",
       "1         0.000855               68900.0  \n",
       "2         0.000452               68690.0  \n",
       "3         0.000451               68480.0  \n",
       "4         0.000731               68270.0  \n",
       "5         0.000496               68060.0  \n",
       "6         0.000479               67850.0  \n",
       "7         0.000735               67640.0  \n",
       "8         0.000491               67430.0  \n",
       "9         0.000466               67220.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(1,df.shape[0]+1))\n",
    "y = df['Accuracy'].values\n",
    "\n",
    "plt.xlabel(\"Averages\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(x,y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s16/s1645821/.conda/envs/asr_env/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+10, 2.30258509e+00, 1.60943791e+00, 1.20397280e+00,\n",
       "       9.16290732e-01, 6.93147181e-01, 5.10825624e-01, 3.56674944e-01,\n",
       "       2.23143551e-01, 1.05360516e-01])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_thresholds = np.arange(0,1,0.1)\n",
    "thresholds = -np.log(prob_thresholds)\n",
    "# change threshold to NLL\n",
    "thresholds[0] = MyViterbiDecoder.NLL_ZERO\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_task3(thresholds, prob_thresholds):\n",
    "    total_error_counts = []\n",
    "    total_word_counts = []\n",
    "\n",
    "    total_average_decoder_times = []\n",
    "    total_average_backtrace_times = []\n",
    "\n",
    "    total_forward_computations = []\n",
    "\n",
    "    different_fs_word_table = [WFST_BASELINE]\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        for idx, (f, word_table) in enumerate(different_fs_word_table[:]):\n",
    "            f_error_counts = [0,0,0]\n",
    "            f_word_counts = 0\n",
    "            decoder_times = []\n",
    "            backtrace_times = []\n",
    "            forward_computations = []\n",
    "            # -- Pruning -- Default False, False, None \n",
    "            pruning = [None,None,threshold]\n",
    "\n",
    "            for wav_file in glob.glob('/group/teaching/asr/labs/recordings/*.wav')[:]:    # replace path if using your own\n",
    "\n",
    "                decoder = MyViterbiDecoder(f, wav_file, word_table)\n",
    "\n",
    "                decoder_start_time = time.time()\n",
    "                decoder.decode(pruning=pruning)\n",
    "                decoder_end_time  = time.time()\n",
    "                (state_path, words) = decoder.backtrace()  # you'll need to modify the backtrace() from Lab 4\n",
    "                                                   # to return the words along the best path\n",
    "                backtrace_end_time = time.time()\n",
    "\n",
    "                transcription = read_transcription(wav_file)\n",
    "                error_counts = wer.compute_alignment_errors(transcription, words) #num_subs, num_del, num_ins\n",
    "                word_count = len(transcription.split())\n",
    "\n",
    "                # add up output\n",
    "                f_word_counts += word_count\n",
    "                f_error_counts[0] += error_counts[0]\n",
    "                f_error_counts[1] += error_counts[1]\n",
    "                f_error_counts[2] += error_counts[2]\n",
    "        #         print(error_counts, word_count)     # you'll need to accumulate these to produce an overall Word Error Rate\n",
    "\n",
    "                # add up times\n",
    "                decoder_time = decoder_end_time - decoder_start_time\n",
    "                backtrace_time = backtrace_end_time - decoder_end_time\n",
    "\n",
    "                decoder_times.append(decoder_time)\n",
    "                backtrace_times.append(backtrace_time)\n",
    "\n",
    "                # add up conputations\n",
    "                forward_computations.append(decoder.forward_computations)\n",
    "\n",
    "                # -- add to DataFrame\n",
    "    #             print(f\"WFTS {idx}: Transcription: {transcription}\\nWords: {words}\")\n",
    "\n",
    "        total_error_counts.append(f_error_counts)\n",
    "        total_word_counts.append(f_word_counts)\n",
    "\n",
    "        total_average_decoder_times.append(sum(decoder_times)/len(decoder_times))\n",
    "        total_average_backtrace_times.append(sum(backtrace_times)/len(backtrace_times))\n",
    "    #     print(f\"Forward computations for fst {idx}: {forward_computations}\")\n",
    "        total_forward_computations.append(sum(forward_computations)/len(forward_computations))\n",
    "    \n",
    "    \n",
    "    columns=['Threshold', 'S', 'D', 'I', 'Accuracy','Word Counts', 'Decoder Times', 'Backtrace Times', 'Forward Computations']\n",
    "    WFSTs = prob_thresholds\n",
    "    total_error_counts = np.array(total_error_counts)\n",
    "    subs = total_error_counts[:,0]\n",
    "    deletions = total_error_counts[:,1]\n",
    "    insertions = total_error_counts[:,2]\n",
    "    accuracies = ((subs + deletions + insertions)/total_word_counts)*100\n",
    "    df = pd.DataFrame((WFSTs, subs, deletions, insertions, accuracies, total_word_counts, total_average_decoder_times, total_average_backtrace_times, total_forward_computations),index=columns).T\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test different thresholds because > 0.1 seems to be really bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s16/s1645821/.conda/envs/asr_env/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>S</th>\n",
       "      <th>D</th>\n",
       "      <th>I</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Word Counts</th>\n",
       "      <th>Decoder Times</th>\n",
       "      <th>Backtrace Times</th>\n",
       "      <th>Forward Computations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>61.430575</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.255427</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>77308.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.058268</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>40.435714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.060967</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>34.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.058078</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>30.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.060141</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>27.742857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.060166</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>25.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>23.814286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.058013</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.058153</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>20.171429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.056493</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Threshold      S       D      I    Accuracy  Word Counts  Decoder Times  \\\n",
       "0        0.0  441.0    60.0  813.0   61.430575       2139.0       2.255427   \n",
       "1        0.1    0.0  2139.0    0.0  100.000000       2139.0       0.058268   \n",
       "2        0.2    0.0  2139.0    0.0  100.000000       2139.0       0.060967   \n",
       "3        0.3    0.0  2139.0    0.0  100.000000       2139.0       0.058078   \n",
       "4        0.4    0.0  2139.0    0.0  100.000000       2139.0       0.060141   \n",
       "5        0.5    0.0  2139.0    0.0  100.000000       2139.0       0.060166   \n",
       "6        0.6    0.0  2139.0    0.0  100.000000       2139.0       0.057900   \n",
       "7        0.7    0.0  2139.0    0.0  100.000000       2139.0       0.058013   \n",
       "8        0.8    0.0  2139.0    0.0  100.000000       2139.0       0.058153   \n",
       "9        0.9    0.0  2139.0    0.0  100.000000       2139.0       0.056493   \n",
       "\n",
       "   Backtrace Times  Forward Computations  \n",
       "0         0.000759          77308.500000  \n",
       "1         0.000450             40.435714  \n",
       "2         0.000473             34.450000  \n",
       "3         0.000474             30.700000  \n",
       "4         0.000479             27.742857  \n",
       "5         0.000461             25.857143  \n",
       "6         0.000458             23.814286  \n",
       "7         0.000617             21.200000  \n",
       "8         0.000456             20.171429  \n",
       "9         0.000440             20.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_thresholds = np.arange(0,1,0.1)\n",
    "thresholds = -np.log(prob_thresholds)\n",
    "# change threshold to NLL\n",
    "thresholds[0] = MyViterbiDecoder.NLL_ZERO\n",
    "\n",
    "df = run_task3(thresholds, prob_thresholds)\n",
    "df.to_excel(\"data/prunning_threshold_sparse.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[0.0, 0.1, 0.01, 0.001, 0.0001, 1e-05, 1e-06, 1e-07, 1e-08, 1e-09, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15, 1e-16, 1e-17, 1e-18, 1e-19, 1e-20, 1e-21, 1e-22, 1.0000000000000001e-23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s16/s1645821/.conda/envs/asr_env/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>S</th>\n",
       "      <th>D</th>\n",
       "      <th>I</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Word Counts</th>\n",
       "      <th>Decoder Times</th>\n",
       "      <th>Backtrace Times</th>\n",
       "      <th>Forward Computations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>441.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>61.430575</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>2.228936</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>77308.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.060559</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>40.435714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.060150</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>58.878571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.060223</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>77.907143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>104.521429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.063376</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>131.807143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.064488</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>152.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.062317</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>174.807143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.064016</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>200.307143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.065835</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>224.935714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.066194</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>254.978571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000e-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.067324</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>282.721429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.066380</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>311.164286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000e-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.068108</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>340.671429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000e-14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.070603</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>370.392857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000e-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.068548</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>404.921429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000e-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.068985</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>441.592857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000e-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>477.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000e-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2137.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.906498</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.074020</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>513.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000e-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2137.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.906498</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.081343</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>549.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000e-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2137.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.906498</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.078633</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>593.292857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000e-21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2137.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.906498</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.076062</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>634.328571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000e-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.812997</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.078532</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>678.085714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000e-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.766246</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.080744</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>726.357143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Threshold      S       D      I    Accuracy  Word Counts  \\\n",
       "0   0.000000e+00  441.0    60.0  813.0   61.430575       2139.0   \n",
       "1   1.000000e-01    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "2   1.000000e-02    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "3   1.000000e-03    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "4   1.000000e-04    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "5   1.000000e-05    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "6   1.000000e-06    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "7   1.000000e-07    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "8   1.000000e-08    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "9   1.000000e-09    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "10  1.000000e-10    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "11  1.000000e-11    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "12  1.000000e-12    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "13  1.000000e-13    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "14  1.000000e-14    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "15  1.000000e-15    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "16  1.000000e-16    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "17  1.000000e-17    0.0  2139.0    0.0  100.000000       2139.0   \n",
       "18  1.000000e-18    0.0  2137.0    0.0   99.906498       2139.0   \n",
       "19  1.000000e-19    0.0  2137.0    0.0   99.906498       2139.0   \n",
       "20  1.000000e-20    0.0  2137.0    0.0   99.906498       2139.0   \n",
       "21  1.000000e-21    0.0  2137.0    0.0   99.906498       2139.0   \n",
       "22  1.000000e-22    0.0  2135.0    0.0   99.812997       2139.0   \n",
       "23  1.000000e-23    0.0  2134.0    0.0   99.766246       2139.0   \n",
       "\n",
       "    Decoder Times  Backtrace Times  Forward Computations  \n",
       "0        2.228936         0.000735          77308.500000  \n",
       "1        0.060559         0.000476             40.435714  \n",
       "2        0.060150         0.000465             58.878571  \n",
       "3        0.060223         0.000487             77.907143  \n",
       "4        0.062500         0.000459            104.521429  \n",
       "5        0.063376         0.000497            131.807143  \n",
       "6        0.064488         0.000471            152.892857  \n",
       "7        0.062317         0.000460            174.807143  \n",
       "8        0.064016         0.000455            200.307143  \n",
       "9        0.065835         0.000465            224.935714  \n",
       "10       0.066194         0.000478            254.978571  \n",
       "11       0.067324         0.000458            282.721429  \n",
       "12       0.066380         0.000608            311.164286  \n",
       "13       0.068108         0.000470            340.671429  \n",
       "14       0.070603         0.000462            370.392857  \n",
       "15       0.068548         0.000432            404.921429  \n",
       "16       0.068985         0.000437            441.592857  \n",
       "17       0.072600         0.000445            477.850000  \n",
       "18       0.074020         0.000485            513.000000  \n",
       "19       0.081343         0.000569            549.750000  \n",
       "20       0.078633         0.000559            593.292857  \n",
       "21       0.076062         0.000441            634.328571  \n",
       "22       0.078532         0.000443            678.085714  \n",
       "23       0.080744         0.000505            726.357143  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end = 1e-24\n",
    "prob_thresholds = np.ones(24)\n",
    "print(prob_thresholds)\n",
    "prob_thresholds[0] = 0\n",
    "prob_thresholds = [x / (10**i) for i,x in enumerate(prob_thresholds)]\n",
    "print(prob_thresholds)\n",
    "thresholds = -np.log(prob_thresholds)\n",
    "# change threshold to NLL\n",
    "thresholds[0] = MyViterbiDecoder.NLL_ZERO\n",
    "\n",
    "df= run_task3(thresholds, prob_thresholds)\n",
    "df.to_excel(\"data/prunning_threshold_fine.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram WFST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "novel-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting bigram probabbilities\n",
    "(dict_first, dict_last, dict_all) = get_word_occurences(transcripts)\n",
    "bigram_df = get_bigram_df(transcripts)\n",
    "df_bigram_prob = get_bigram_prob_df(bigram_df, dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "illegal-event",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting the bigram fst\n",
    "lex = 'lexicon.txt'\n",
    "WFST_BIGRAM_F_5_5 = generate_word_sequence_recognition_wfst_bigram(1, lex, df_bigram_prob, original=False, weight_fwd=0.5, weight_self=0.5)\n",
    "WFST_BIGRAM_F_3_7 = generate_word_sequence_recognition_wfst_bigram(1, lex, df_bigram_prob, original=False, weight_fwd=0.3, weight_self=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pic(WFST_BIGRAM_F_5_5[0],'WFST_BIGRAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_error_counts = []\n",
    "total_word_counts = []\n",
    "\n",
    "total_average_decoder_times = []\n",
    "total_average_backtrace_times = []\n",
    "\n",
    "total_forward_computations = []\n",
    "# -- lsit of wfst(s) from previous tasks\n",
    "list_wfst_task1 = [WFST_1_9_O, WFST_1_9, WFST_O, WFST_multi]\n",
    "list_wfst_task2 = [WFST_silent, WFST_unigram, WFST_3_7, WFST_final_prob]\n",
    "list_wfst_task4 = [WFST_BIGRAM_F_5_5, WFST_BIGRAM_F_3_7] \n",
    "# -- wfst(s) used\n",
    "# different_fs_word_table = list_wfst_task4 + list_wfst_task2 + list_wfst_task1\n",
    "different_fs_word_table = [WFST_BIGRAM_F_5_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running WFST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n",
      "doing regular step forward\n"
     ]
    }
   ],
   "source": [
    "for idx, (f, word_table) in enumerate(different_fs_word_table[:]):\n",
    "    f_error_counts = [0,0,0]\n",
    "    f_word_counts = 0\n",
    "    decoder_times = []\n",
    "    backtrace_times = []\n",
    "    forward_computations = []\n",
    "    # -- Pruning -- Default False, False, None \n",
    "    pruning = [None,None,None]\n",
    "    \n",
    "    for wav_file in glob.glob('/group/teaching/asr/labs/recordings/*.wav')[:]:    # replace path if using your own\n",
    "        \n",
    "        decoder = MyViterbiDecoder(f, wav_file, word_table)\n",
    "        \n",
    "        decoder_start_time = time.time()\n",
    "        decoder.decode(pruning=pruning)\n",
    "        decoder_end_time  = time.time()\n",
    "        (state_path, words) = decoder.backtrace()  # you'll need to modify the backtrace() from Lab 4\n",
    "                                           # to return the words along the best path\n",
    "        backtrace_end_time = time.time()\n",
    "        \n",
    "        transcription = read_transcription(wav_file)\n",
    "        error_counts = wer.compute_alignment_errors(transcription, words) #num_subs, num_del, num_ins\n",
    "        word_count = len(transcription.split())\n",
    "        \n",
    "        # add up output\n",
    "        f_word_counts += word_count\n",
    "        f_error_counts[0] += error_counts[0]\n",
    "        f_error_counts[1] += error_counts[1]\n",
    "        f_error_counts[2] += error_counts[2]\n",
    "#         print(error_counts, word_count)     # you'll need to accumulate these to produce an overall Word Error Rate\n",
    "        \n",
    "        # add up times\n",
    "        decoder_time = decoder_end_time - decoder_start_time\n",
    "        backtrace_time = backtrace_end_time - decoder_end_time\n",
    "        \n",
    "        decoder_times.append(decoder_time)\n",
    "        backtrace_times.append(backtrace_time)\n",
    "        \n",
    "        # add up conputations\n",
    "        forward_computations.append(decoder.forward_computations)\n",
    "        \n",
    "        # -- add to DataFrame\n",
    "#         print(f\"WFTS {idx}: Transcription: {transcription}\\nWords: {words}\")\n",
    "        \n",
    "    total_error_counts.append(f_error_counts)\n",
    "    total_word_counts.append(f_word_counts)\n",
    "    \n",
    "    total_average_decoder_times.append(sum(decoder_times)/len(decoder_times))\n",
    "    total_average_backtrace_times.append(sum(backtrace_times)/len(backtrace_times))\n",
    "#     print(f\"Forward computations for fst {idx}: {forward_computations}\")\n",
    "    total_forward_computations.append(sum(forward_computations)/len(forward_computations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_wfst_task1 = [WFST_1_9_O, WFST_1_9, WFST_O, WFST_multi]\n",
    "# list_wfst_task2 = [WFST_silent, WFST_unigram, WFST_3_7, WFST_final_prob]\n",
    "# list_wfst_task4 = [WFST_BIGRAM_F_5_5, WFST_BIGRAM_F_3_7] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['WFST', 'S', 'D', 'I', 'Accuracy','Word Counts', 'Decoder Times', 'Backtrace Times', 'Forward Computations']\n",
    "# WFSTs = ['WFST_BIGRAM_F_5_5', 'WFST_BIGRAM_F_3_7', 'WFST_silent','WFST_unigram', 'WFST_3_7', 'WFST_final_prob', 'WFST_1_9_O', 'WFST_1_9', 'WFST_O', 'WFST_multi']\n",
    "WFSTs = ['WFST_BIGRAM_F_5_5'] \n",
    "total_error_counts = np.array(total_error_counts)\n",
    "subs = total_error_counts[:,0]\n",
    "deletions = total_error_counts[:,1]\n",
    "insertions = total_error_counts[:,2]\n",
    "accuracies = ((subs + deletions + insertions)/total_word_counts)*100\n",
    "df = pd.DataFrame((WFSTs, subs, deletions, insertions, accuracies, total_word_counts, total_average_decoder_times, total_average_backtrace_times, total_forward_computations),index=columns).T\n",
    "# df.to_excel('task4_bigram.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WFST</th>\n",
       "      <th>S</th>\n",
       "      <th>D</th>\n",
       "      <th>I</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Word Counts</th>\n",
       "      <th>Decoder Times</th>\n",
       "      <th>Backtrace Times</th>\n",
       "      <th>Forward Computations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WFST_BIGRAM_F_5_5</td>\n",
       "      <td>451</td>\n",
       "      <td>2</td>\n",
       "      <td>3092</td>\n",
       "      <td>165.73165</td>\n",
       "      <td>2139</td>\n",
       "      <td>1.366138</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>27868.314286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                WFST    S  D     I   Accuracy Word Counts Decoder Times  \\\n",
       "0  WFST_BIGRAM_F_5_5  451  2  3092  165.73165        2139      1.366138   \n",
       "\n",
       "  Backtrace Times Forward Computations  \n",
       "0        0.000852         27868.314286  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie = Trie('lexicon_tree_baseline.txt')\n",
    "trie_wfst = trie.f\n",
    "trie_word_table = trie.word_table\n",
    "\n",
    "trie_wfst_word_table = (trie_wfst,trie_word_table )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "WFSTs_word_table = [trie_wfst_word_table]\n",
    "\n",
    "total_error_counts = []\n",
    "total_word_counts = []\n",
    "\n",
    "total_average_decoder_times = []\n",
    "total_average_backtrace_times = []\n",
    "\n",
    "total_forward_computations = []\n",
    "\n",
    "for f, word_table in WFSTs_word_table[:]:\n",
    "    f_error_counts = [0,0,0]\n",
    "    f_word_counts = 0\n",
    "    decoder_times = []\n",
    "    backtrace_times = []\n",
    "    forward_computations = []    \n",
    "    \n",
    "    for wav_file in glob.glob('/group/teaching/asr/labs/recordings/*.wav')[:]:    # replace path if using your own\n",
    "        \n",
    "        decoder = MyViterbiDecoder(f, wav_file, word_table)\n",
    "        \n",
    "        decoder_start_time = time.time()\n",
    "        decoder.decode()\n",
    "        decoder_end_time  = time.time()\n",
    "        (state_path, words) = decoder.backtrace()  # you'll need to modify the backtrace() from Lab 4\n",
    "                                           # to return the words along the best path\n",
    "        backtrace_end_time = time.time()\n",
    "        \n",
    "        transcription = read_transcription(wav_file)\n",
    "        \n",
    "        # remove silence from words as this doesnt count in transcription\n",
    "#         if ('<silence>' in words):\n",
    "#             words = words.replace('<silence>', '')\n",
    "#             words = words[1:-1]\n",
    "        \n",
    "        error_counts = wer.compute_alignment_errors(transcription, words) #num_subs, num_del, num_ins\n",
    "        word_count = len(transcription.split())\n",
    "        \n",
    "#         print(f\"Transcription: {transcription}\\nWords: {words}\")\n",
    "        \n",
    "        # add up output\n",
    "        f_word_counts += word_count\n",
    "        f_error_counts[0] += error_counts[0]\n",
    "        f_error_counts[1] += error_counts[1]\n",
    "        f_error_counts[2] += error_counts[2]\n",
    "#         print(error_counts, word_count)     # you'll need to accumulate these to produce an overall Word Error Rate\n",
    "        \n",
    "        # add up times\n",
    "        decoder_time = decoder_end_time - decoder_start_time\n",
    "        backtrace_time = backtrace_end_time - decoder_end_time\n",
    "        \n",
    "        decoder_times.append(decoder_time)\n",
    "        backtrace_times.append(backtrace_time)\n",
    "        \n",
    "        # add up conputations\n",
    "        forward_computations.append(decoder.forward_computations)\n",
    "        \n",
    "        # -- add to DataFrame\n",
    "#         wav_name = wav_file.split('\\\\')[-1]\n",
    "#         pd_row = []\n",
    "        \n",
    "    total_error_counts.append(f_error_counts)\n",
    "    total_word_counts.append(f_word_counts)\n",
    "    \n",
    "    total_average_decoder_times.append(sum(decoder_times)/len(decoder_times))\n",
    "    total_average_backtrace_times.append(sum(backtrace_times)/len(backtrace_times))\n",
    "    \n",
    "    total_forward_computations.append(sum(forward_computations)/len(forward_computations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WFST</th>\n",
       "      <th>S</th>\n",
       "      <th>D</th>\n",
       "      <th>I</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Word Counts</th>\n",
       "      <th>Decoder Times</th>\n",
       "      <th>Backtrace Times</th>\n",
       "      <th>Forward Computations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tree baseline</td>\n",
       "      <td>441</td>\n",
       "      <td>60</td>\n",
       "      <td>813</td>\n",
       "      <td>61.430575</td>\n",
       "      <td>2139</td>\n",
       "      <td>2.10464</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>58575.985714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            WFST    S   D    I   Accuracy Word Counts Decoder Times  \\\n",
       "0  Tree baseline  441  60  813  61.430575        2139       2.10464   \n",
       "\n",
       "  Backtrace Times Forward Computations  \n",
       "0        0.000769         58575.985714  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=['WFST', 'S', 'D', 'I', 'Accuracy','Word Counts', 'Decoder Times', 'Backtrace Times', 'Forward Computations']\n",
    "WFSTs = ['Tree baseline']\n",
    "total_error_counts = np.array(total_error_counts)\n",
    "subs = total_error_counts[:,0]\n",
    "deletions = total_error_counts[:,1]\n",
    "insertions = total_error_counts[:,2]\n",
    "accuracies = ((subs + deletions + insertions)/total_word_counts)*100\n",
    "df = pd.DataFrame((WFSTs, subs, deletions, insertions, accuracies, total_word_counts, total_average_decoder_times, total_average_backtrace_times, total_forward_computations),index=columns).T\n",
    "df.to_excel('task4_tree_baseline.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
